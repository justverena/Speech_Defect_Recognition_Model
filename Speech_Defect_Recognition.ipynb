{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b933bd1a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e3399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2Processor\n",
    "INPUT_DIR = \"dataset_raw\"\n",
    "OUTPUT_DIR = \"dataset\"\n",
    "DATA_DIR = \"dataset\"\n",
    "classes = [\"normal\", \"articulation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ae85f0",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## 1. Project Objective\n",
    "#### The goal of this project is to develop a model capable of automatically classifying speech defects from short audio samples.  \n",
    "\n",
    "The initial prototype focuses on binary classification of:\n",
    "- **normal speech**\n",
    "- **articulation disorders** (e.g., mispronunciations, phonetic deviations)\n",
    "- **stuttering**, (CLASS / SEP-28k).  \n",
    "\n",
    "## 2. Datasets Used\n",
    "\n",
    "For this project we rely on two primary sources:\n",
    "\n",
    "### 1. Articulation Disorders Dataset  \n",
    "Contains recordings of subjects with various articulation impairments.  \n",
    "Directory example:\n",
    "\n",
    "`dataset_raw/articulation/M05/Session1/wav_arrayMic/0001.wav`\n",
    "\n",
    "### 2. Normal Speech Dataset (LibriSpeech Subset) \n",
    "Contains clean speech samples from speakers without speech impairments.  \n",
    "Directory example:\n",
    "\n",
    "`dataset_raw/normal/dev-clean/batch1/1993/147964/1993-147964-0000.wav`\n",
    "\n",
    "### 3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a4985d",
   "metadata": {},
   "source": [
    "# Dataset Exploration\n",
    "## Printing directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c9adfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_raw\n",
      " Subdirectories: ['stuttering', 'articulation', 'normal']\n"
     ]
    }
   ],
   "source": [
    "for root, dirs, files in os.walk(INPUT_DIR):\n",
    "    print(root)\n",
    "    print(\" Subdirectories:\", dirs)\n",
    "    break  # printing only top-level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c2843b",
   "metadata": {},
   "source": [
    "## Inspecting number of files per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b3824bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal speech files: 5930\n",
      "Articulation disorder files: 6179\n",
      "Stuttering files: 6092\n"
     ]
    }
   ],
   "source": [
    "def count_audio_files(path):\n",
    "    count = 0\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        count += len([f for f in files if f.lower().endswith(\".wav\")])\n",
    "    return count\n",
    "\n",
    "normal_count = count_audio_files(os.path.join(INPUT_DIR, \"normal\"))\n",
    "articulation_count = count_audio_files(os.path.join(INPUT_DIR, \"articulation\"))\n",
    "stuttering_count = count_audio_files(os.path.join(INPUT_DIR, \"stuttering\"))\n",
    "\n",
    "print(\"Normal speech files:\", normal_count)\n",
    "print(\"Articulation disorder files:\", articulation_count)\n",
    "print(\"Stuttering files:\", stuttering_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61334a84",
   "metadata": {},
   "source": [
    "## Loading and displaying waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d569850",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = \"dataset_raw/normal/dev-clean/1272/141231/1272-141231-0000.wav\"\n",
    "\n",
    "audio, sr = librosa.load(sample_path, sr=None, mono=True)\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "librosa.display.waveshow(audio, sr=sr)\n",
    "plt.title(f\"Waveform — {os.path.basename(sample_path)}\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Sampling rate:\", sr)\n",
    "print(\"Duration (sec):\", len(audio) / sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570d214a",
   "metadata": {},
   "source": [
    "## Playing audio sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3ccb820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/x-wav;base64,dmVyc2lvbiBodHRwczovL2dpdC1sZnMuZ2l0aHViLmNvbS9zcGVjL3YxCm9pZCBzaGEyNTY6NjJlZDgwODYyNGU4ZTA3Yzc2ODE3Y2NhMDliN2E5YzE2ZmExNmNlMTBmZjU0OTUyMmQyODA0ODNmNjY2N2ZhYQpzaXplIDE0ODg3OAo=\" type=\"audio/x-wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Audio(sample_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbf933f",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "## Converting to mono and resampling (16 kHz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "653add62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path, target_sr=16000):\n",
    "    try:\n",
    "        audio, sr = librosa.load(path, sr=target_sr, mono=True)\n",
    "        return audio, target_sr\n",
    "    except Exception as e:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db00d31",
   "metadata": {},
   "source": [
    "## RMS normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9436a7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_audio(audio):\n",
    "    rms = np.sqrt(np.mean(audio**2))\n",
    "    target_rms = 0.03  # -30 dB\n",
    "    if rms > 0:\n",
    "        audio = audio * (target_rms / rms)\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ace0ab2",
   "metadata": {},
   "source": [
    "## Cutting into 3-5 second Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97a38a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio(audio, sr, min_sec=3, max_sec=5):\n",
    "    chunk_len = np.random.randint(min_sec, max_sec + 1) * sr\n",
    "    chunks = []\n",
    "\n",
    "    for start in range(0, len(audio), chunk_len):\n",
    "        end = start + chunk_len\n",
    "        chunk = audio[start:end]\n",
    "        if len(chunk) < min_sec * sr:\n",
    "            continue\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0844e639",
   "metadata": {},
   "source": [
    "## Single file processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eacd9457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(input_path, output_dir, label, errors_log):\n",
    "    audio, sr = load_audio(input_path)\n",
    "\n",
    "    if audio is None:\n",
    "        errors_log.append(f\"Failed to load: {input_path}\")\n",
    "        return\n",
    "\n",
    "    audio = normalize_audio(audio)\n",
    "    chunks = split_audio(audio, sr)\n",
    "\n",
    "    if len(chunks) == 0:\n",
    "        errors_log.append(f\"No suitable chunks: {input_path}\")\n",
    "        return\n",
    "\n",
    "    # creating directory\n",
    "    class_dir = os.path.join(output_dir, label)\n",
    "    os.makedirs(class_dir, exist_ok=True)\n",
    "\n",
    "    # saving chunks\n",
    "    base_name = os.path.splitext(os.path.basename(input_path))[0]\n",
    "\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        out_path = os.path.join(class_dir, f\"{base_name}_{idx}.wav\")\n",
    "        sf.write(out_path, chunk, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3ca5dd",
   "metadata": {},
   "source": [
    "## Main function for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab6e6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing normal: 0it [00:00, ?it/s]\n",
      "Processing normal: 0it [00:00, ?it/s]\n",
      "Processing normal: 0it [00:00, ?it/s]\n",
      "Processing normal: 0it [00:00, ?it/s]\n",
      "Processing normal: 100%|██████████| 53/53 [00:00<00:00, 318.91it/s]\n",
      "Processing normal: 0it [00:00, ?it/s]\n",
      "Processing normal: 100%|██████████| 30/30 [00:00<00:00, 578.22it/s]\n",
      "Processing normal: 100%|██████████| 31/31 [00:00<00:00, 447.19it/s]\n",
      "Processing normal: 100%|██████████| 17/17 [00:00<00:00, 287.75it/s]\n",
      "Processing normal: 0it [00:00, ?it/s]\n",
      "Processing normal: 100%|██████████| 65/65 [00:00<00:00, 396.07it/s]\n",
      "Processing normal: 0it [00:00, ?it/s]\n",
      "Processing normal: 100%|██████████| 10/10 [00:00<00:00, 488.40it/s]\n",
      "Processing normal: 100%|██████████| 12/12 [00:00<00:00, 403.50it/s]\n",
      "Processing normal: 100%|██████████| 8/8 [00:00<00:00, 526.10it/s]\n",
      "Processing normal: 100%|██████████| 32/32 [00:00<00:00, 342.88it/s]\n",
      "Processing normal: 0it [00:00, ?it/s]\n",
      "Processing normal: 100%|██████████| 24/24 [00:00<00:00, 403.32it/s]\n",
      "Processing normal: 100%|██████████| 21/21 [00:00<00:00, 448.11it/s]\n",
      "Processing normal: 100%|██████████| 17/17 [00:00<00:00, 325.47it/s]\n",
      "Processing normal: 0it [00:00, ?it/s]\n",
      "Processing normal: 100%|██████████| 29/29 [00:00<00:00, 412.07it/s]\n",
      "Processing normal: 100%|██████████| 24/24 [00:00<00:00, 614.61it/s]\n",
      "Processing normal: 100%|██████████| 44/44 [00:00<00:00, 589.09it/s]\n",
      "Processing normal: 0it [00:00, ?it/s]\n",
      "Processing normal: 100%|██████████| 26/26 [00:00<00:00, 291.10it/s]\n",
      "Processing normal: 100%|██████████| 6/6 [00:00<00:00, 335.89it/s]\n",
      "Processing normal: 0it [00:00, ?it/s]\n",
      "Processing normal: 100%|██████████| 34/34 [00:00<00:00, 469.04it/s]\n",
      "Processing normal: 100%|██████████| 16/16 [00:00<00:00, 332.89it/s]\n",
      "Processing normal: 100%|██████████| 26/26 [00:00<00:00, 511.29it/s]\n",
      "Processing normal: 0it [00:00, ?it/s]\n",
      "Processing normal: 100%|██████████| 22/22 [00:00<00:00, 295.64it/s]\n",
      "Processing normal: 100%|██████████| 22/22 [00:00<00:00, 246.29it/s]\n",
      "Processing normal: 0it [00:00, ?it/s]\n",
      "Processing normal: 100%|██████████| 42/42 [00:00<00:00, 564.97it/s]\n",
      "Processing normal: 100%|██████████| 18/18 [00:00<00:00, 190.03it/s]\n",
      "Processing normal: 100%|██████████| 20/20 [00:00<00:00, 295.77it/s]\n",
      "Processing articulation: 0it [00:00, ?it/s]\n",
      "Processing articulation: 0it [00:00, ?it/s]\n",
      "Processing articulation: 0it [00:00, ?it/s]\n",
      "Processing articulation: 100%|██████████| 100/100 [00:00<00:00, 486.34it/s]\n",
      "Processing articulation: 100%|██████████| 100/100 [00:00<00:00, 537.36it/s]\n",
      "Processing articulation: 0it [00:00, ?it/s]\n",
      "Processing articulation: 100%|██████████| 286/286 [00:00<00:00, 712.37it/s]\n",
      "Processing articulation: 100%|██████████| 286/286 [00:00<00:00, 706.18it/s]\n",
      "Processing articulation: 0it [00:00, ?it/s]\n",
      "Processing articulation: 0it [00:00, ?it/s]\n",
      "Processing articulation: 100%|██████████| 209/209 [00:00<00:00, 920.87it/s] \n",
      "Processing articulation: 100%|██████████| 214/214 [00:00<00:00, 1000.82it/s]\n",
      "Processing articulation: 0it [00:00, ?it/s]\n",
      "Processing articulation: 100%|██████████| 435/435 [00:00<00:00, 930.82it/s]\n",
      "Processing articulation: 100%|██████████| 159/159 [00:00<00:00, 813.32it/s]\n",
      "Processing articulation: 0it [00:00, ?it/s]\n",
      "Processing articulation: 100%|██████████| 204/204 [00:00<00:00, 954.86it/s]\n",
      "Processing articulation: 100%|██████████| 204/204 [00:00<00:00, 814.21it/s]\n",
      "Processing articulation: 0it [00:00, ?it/s]\n",
      "Processing articulation: 0it [00:00, ?it/s]\n",
      "Processing articulation: 100%|██████████| 249/249 [00:00<00:00, 651.82it/s]\n",
      "Processing articulation: 100%|██████████| 250/250 [00:00<00:00, 725.99it/s]\n",
      "Processing articulation: 0it [00:00, ?it/s]\n",
      "Processing articulation: 100%|██████████| 199/199 [00:00<00:00, 916.34it/s]\n",
      "Processing articulation: 0it [00:00, ?it/s]\n",
      "Processing articulation: 0it [00:00, ?it/s]\n",
      "Processing articulation: 100%|██████████| 295/295 [00:00<00:00, 680.56it/s]\n",
      "Processing articulation: 100%|██████████| 298/298 [00:00<00:00, 617.66it/s]\n",
      "Processing articulation: 0it [00:00, ?it/s]\n",
      "Processing articulation: 100%|██████████| 126/126 [00:00<00:00, 671.24it/s]\n",
      "Processing articulation: 0it [00:00, ?it/s]\n",
      "Processing articulation: 0it [00:00, ?it/s]\n",
      "Processing articulation: 100%|██████████| 416/416 [00:00<00:00, 695.22it/s]\n",
      "Processing articulation: 100%|██████████| 421/421 [00:00<00:00, 821.74it/s]\n",
      "Processing articulation: 0it [00:00, ?it/s]\n",
      "Processing articulation: 0it [00:00, ?it/s]\n",
      "Processing articulation: 100%|██████████| 160/160 [00:00<00:00, 607.44it/s]\n",
      "Processing articulation: 100%|██████████| 169/169 [00:00<00:00, 618.95it/s]\n",
      "Processing articulation: 0it [00:00, ?it/s]\n",
      "Processing articulation: 100%|██████████| 240/240 [00:00<00:00, 840.62it/s]\n",
      "Processing articulation: 100%|██████████| 240/240 [00:00<00:00, 762.03it/s]\n",
      "Processing articulation: 0it [00:00, ?it/s]\n",
      "Processing articulation: 0it [00:00, ?it/s]\n",
      "Processing articulation: 100%|██████████| 393/393 [00:00<00:00, 623.65it/s]\n",
      "Processing articulation: 0it [00:00, ?it/s]\n",
      "Processing articulation: 100%|██████████| 128/128 [00:00<00:00, 628.40it/s]\n",
      "Processing articulation: 100%|██████████| 130/130 [00:00<00:00, 619.29it/s]\n",
      "Processing articulation: 0it [00:00, ?it/s]\n",
      "Processing articulation: 100%|██████████| 134/134 [00:00<00:00, 977.17it/s] \n",
      "Processing articulation: 100%|██████████| 134/134 [00:00<00:00, 977.26it/s] \n"
     ]
    }
   ],
   "source": [
    "def preprocess_dataset(input_root, output_root):\n",
    "    errors = []\n",
    "    for label in classes:\n",
    "        input_path = os.path.join(input_root, label)\n",
    "\n",
    "        if not os.path.exists(input_path):\n",
    "            print(f\"Missing folder: {input_path}\")\n",
    "            continue\n",
    "        \n",
    "        # recursively traversing all subfolders\n",
    "        for root, dirs, files in os.walk(input_path):\n",
    "            wav_files = [f for f in files if f.lower().endswith(\".wav\")]\n",
    "\n",
    "            for fname in tqdm(wav_files, desc=f\"Processing {label}\"):\n",
    "                full_path = os.path.join(root, fname)\n",
    "                process_file(full_path, output_root, label, errors)\n",
    "\n",
    "    return errors\n",
    "\n",
    "errors = preprocess_dataset(INPUT_DIR, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dad5ddd",
   "metadata": {},
   "source": [
    "## Saving error logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e86c28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors logged to prep_errors.log\n"
     ]
    }
   ],
   "source": [
    "with open(\"prep_errors.log\", \"w\") as f:\n",
    "    for e in errors:\n",
    "        f.write(e + \"\\n\")\n",
    "\n",
    "print(\"Errors logged to prep_errors.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbc3ca8",
   "metadata": {},
   "source": [
    "# Dataset CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90172641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files: train.csv, val.csv, test.csv\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for label in classes:\n",
    "    class_dir = os.path.join(DATA_DIR, label)\n",
    "    if not os.path.exists(class_dir):\n",
    "        continue\n",
    "    for f in os.listdir(class_dir):\n",
    "        if f.endswith(\".wav\"):\n",
    "            path = os.path.join(class_dir, f)\n",
    "            data.append([path, label])\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"path\", \"label\"])\n",
    "\n",
    "train_val, test = train_test_split(df, test_size=0.1, random_state=42, stratify=df[\"label\"])\n",
    "train, val = train_test_split(train_val, test_size=0.1111, random_state=42, stratify=train_val[\"label\"])  # 10%\n",
    "\n",
    "train.to_csv(\"train.csv\", index=False)\n",
    "val.to_csv(\"val.csv\", index=False)\n",
    "test.to_csv(\"test.csv\", index=False)\n",
    "\n",
    "print(\"CSV files: train.csv, val.csv, test.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7d437a",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c30f14e",
   "metadata": {},
   "source": [
    "## Loader preporation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6e4c897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# innitialization\n",
    "processor = Wav2Vec2Processor.from_pretrained(\n",
    "    \"facebook/wav2vec2-base-960h\"\n",
    ")\n",
    "# adding random noise\n",
    "def add_noise(audio, noise_factor=0.005):\n",
    "    noise = np.random.randn(len(audio))\n",
    "    augmented = audio + noise_factor * noise\n",
    "    return augmented.astype(np.float32)\n",
    "\n",
    "# SpecAugment\n",
    "def spec_augment(features, freq_mask=15, time_mask=35):\n",
    "    # freq masking\n",
    "    f = features.shape[0]\n",
    "    f0 = np.random.randint(0, f - freq_mask)\n",
    "    features[f0:f0 + freq_mask, :] = 0\n",
    "    # time masking\n",
    "    t = features.shape[1]\n",
    "    t0 = np.random.randint(0, t - time_mask)\n",
    "    features[:, t0:t0 + time_mask] = 0\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba98c1c",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b698dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL2ID = {\n",
    "    \"normal\": 0,\n",
    "    \"articulation\": 1,\n",
    "    \"stuttering\": 2\n",
    "}\n",
    "\n",
    "ID2LABEL = {v: k for k, v in LABEL2ID.items()}\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_path,\n",
    "        processor,\n",
    "        target_sr=16000,\n",
    "        augment=False,\n",
    "        max_duration_sec=10\n",
    "    ):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.processor = processor\n",
    "        self.target_sr = target_sr\n",
    "        self.augment = augment\n",
    "        self.max_samples = target_sr * max_duration_sec\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        path = row[\"path\"]\n",
    "        label = LABEL2ID[row[\"label\"]]\n",
    "\n",
    "        # load audio\n",
    "        audio, sr = sf.read(path)\n",
    "        if audio.ndim > 1:\n",
    "            audio = audio.mean(axis=1)\n",
    "\n",
    "        if sr != self.target_sr:\n",
    "            audio = librosa.resample(audio, orig_sr=sr, target_sr=self.target_sr)\n",
    "\n",
    "        audio = audio.astype(np.float32)\n",
    "\n",
    "        # hard cut (important for memory)\n",
    "        if len(audio) > self.max_samples:\n",
    "            audio = audio[: self.max_samples]\n",
    "\n",
    "        # simple augmentation\n",
    "        if self.augment and np.random.rand() < 0.3:\n",
    "            noise = np.random.randn(len(audio)) * 0.003\n",
    "            audio = audio + noise\n",
    "\n",
    "        return {\n",
    "            \"input_values\": audio,\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63235d7",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57f9d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AudioDataset(\"train.csv\", processor, augment=True)\n",
    "val_dataset   = AudioDataset(\"val.csv\", processor, augment=False)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    audios = [item[\"input_values\"] for item in batch]\n",
    "    labels = torch.stack([item[\"labels\"] for item in batch])\n",
    "\n",
    "    batch = processor(\n",
    "        audios,\n",
    "        sampling_rate=16000,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db32469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_values: torch.Size([4, 80000])\n",
      "attention_mask: torch.Size([4, 80000])\n",
      "labels: tensor([0, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(\"input_values:\", batch[\"input_values\"].shape)\n",
    "print(\"attention_mask:\", batch[\"attention_mask\"].shape)\n",
    "print(\"labels:\", batch[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c00c60",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3214e8b",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d222df1f",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
